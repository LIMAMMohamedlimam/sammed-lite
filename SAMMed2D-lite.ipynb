{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c383648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cad31701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "filelist=(\"sam_vit_b_01ec64.pth\" \"sample_data\")\n",
    "\n",
    "pat=$(printf \"^%s$\" \"${filelist[@]}\")\n",
    "pat=${pat:1}\n",
    "\n",
    "\n",
    "ls | grep -Ev \"$pat\" | xargs rm -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1ea8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c13411f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: copied repo content into ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp_repo'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Variables\n",
    "REPO_URL=\"https://github.com/LIMAMMohamedlimam/sammed-lite.git\"\n",
    "CLONE_DIR=\"temp_repo\"\n",
    "TARGET_DIR=\"./\"\n",
    "git clone \"$REPO_URL\" \"$CLONE_DIR\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "mkdir -p \"$TARGET_DIR\"\n",
    "\n",
    "# Copy all contents (including hidden files)\n",
    "cp -r \"$CLONE_DIR\"/. \"$TARGET_DIR\"/\n",
    "\n",
    "# Delete cloned repo directory\n",
    "rm -rf \"$CLONE_DIR\"\n",
    "\n",
    "echo \"Done: copied repo content into $TARGET_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13ef5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "lite-sammed2d.py  SAMMed2D-lite.ipynb  segment_anything\n",
      "\n",
      "./segment_anything:\n",
      "automatic_mask_generator.py  __init__.py  predictor.py\n",
      "build_sam.py\t\t     modeling\t  utils\n",
      "\n",
      "./segment_anything/modeling:\n",
      "common.py\t  __init__.py\t   prompt_encoder.py  transformer.py\n",
      "image_encoder.py  mask_decoder.py  sam.py\n",
      "\n",
      "./segment_anything/utils:\n",
      "amg.py\t__init__.py  onnx.py  transforms.py\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7e35691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling import (\n",
    "    Sam,\n",
    "    ImageEncoderViT,\n",
    "    MaskDecoder,\n",
    "    PromptEncoder,\n",
    "    TwoWayTransformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c84aa32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50332037",
   "metadata": {},
   "source": [
    "## Adapter layer implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2f482",
   "metadata": {},
   "source": [
    "### Custom Transformer Block  (Adapter_layer injected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8670e",
   "metadata": {},
   "source": [
    "## Medical Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebadc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "import random\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def train_transforms(img_size, ori_h, ori_w):\n",
    "    transforms = []\n",
    "    if ori_h < img_size and ori_w < img_size:\n",
    "        transforms.append(A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)))\n",
    "    else:\n",
    "        transforms.append(A.Resize(int(img_size), int(img_size), interpolation=cv2.INTER_NEAREST))\n",
    "    transforms.append(ToTensorV2(p=1.0))\n",
    "    return A.Compose(transforms, p=1.)\n",
    "\n",
    "\n",
    "def get_boxes_from_mask(mask, box_num=1, std = 0.1, max_pixel = 5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mask: Mask, can be a torch.Tensor or a numpy array of binary mask.\n",
    "        box_num: Number of bounding boxes, default is 1.\n",
    "        std: Standard deviation of the noise, default is 0.1.\n",
    "        max_pixel: Maximum noise pixel value, default is 5.\n",
    "    Returns:\n",
    "        noise_boxes: Bounding boxes after noise perturbation, returned as a torch.Tensor.\n",
    "    \"\"\"\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.numpy()\n",
    "        \n",
    "    label_img = label(mask)\n",
    "    regions = regionprops(label_img)\n",
    "\n",
    "    # Iterate through all regions and get the bounding box coordinates\n",
    "    boxes = [tuple(region.bbox) for region in regions]\n",
    "\n",
    "    # If the generated number of boxes is greater than the number of categories,\n",
    "    # sort them by region area and select the top n regions\n",
    "    if len(boxes) >= box_num:\n",
    "        sorted_regions = sorted(regions, key=lambda x: x.area, reverse=True)[:box_num]\n",
    "        boxes = [tuple(region.bbox) for region in sorted_regions]\n",
    "\n",
    "    # If the generated number of boxes is less than the number of categories,\n",
    "    # duplicate the existing boxes\n",
    "    elif len(boxes) < box_num:\n",
    "        num_duplicates = box_num - len(boxes)\n",
    "        boxes += [boxes[i % len(boxes)] for i in range(num_duplicates)]\n",
    "\n",
    "    # Perturb each bounding box with noise\n",
    "    noise_boxes = []\n",
    "    for box in boxes:\n",
    "        y0, x0,  y1, x1  = box\n",
    "        width, height = abs(x1 - x0), abs(y1 - y0)\n",
    "        # Calculate the standard deviation and maximum noise value\n",
    "        noise_std = min(width, height) * std\n",
    "        max_noise = min(max_pixel, int(noise_std * 5))\n",
    "         # Add random noise to each coordinate\n",
    "        try:\n",
    "            noise_x = np.random.randint(-max_noise, max_noise)\n",
    "        except:\n",
    "            noise_x = 0\n",
    "        try:\n",
    "            noise_y = np.random.randint(-max_noise, max_noise)\n",
    "        except:\n",
    "            noise_y = 0\n",
    "        x0, y0 = x0 + noise_x, y0 + noise_y\n",
    "        x1, y1 = x1 + noise_x, y1 + noise_y\n",
    "        noise_boxes.append((x0, y0, x1, y1))\n",
    "    return torch.as_tensor(noise_boxes, dtype=torch.float)\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=256, mode='train', requires_name=True, point_num=1, mask_num=5):\n",
    "        \"\"\"\n",
    "        Initializes a training dataset.\n",
    "        Args:\n",
    "            data_dir (str): Directory containing the dataset.\n",
    "            image_size (int, optional): Desired size for the input images. Defaults to 256.\n",
    "            mode (str, optional): Mode of the dataset. Defaults to 'train'.\n",
    "            requires_name (bool, optional): Indicates whether to include image names in the output. Defaults to True.\n",
    "            num_points (int, optional): Number of points to sample. Defaults to 1.\n",
    "            num_masks (int, optional): Number of masks to sample. Defaults to 5.\n",
    "        \"\"\"\n",
    "        self.image_size = image_size\n",
    "        self.requires_name = requires_name\n",
    "        self.point_num = point_num\n",
    "        self.mask_num = mask_num\n",
    "        self.pixel_mean = [123.675, 116.28, 103.53]\n",
    "        self.pixel_std = [58.395, 57.12, 57.375]\n",
    "\n",
    "        dataset = json.load(open(os.path.join(data_dir, f'image2label_{mode}.json'), \"r\"))\n",
    "        for i in list(dataset.keys()):\n",
    "            self.image_paths.append(i.replace(\"data_demo/images/\" , \"images_dir/\")) \n",
    "        self.label_paths = list(dataset.values())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "        Returns:\n",
    "            dict: A dictionary containing the sample data.\n",
    "        \"\"\"\n",
    "\n",
    "        image_input = {}\n",
    "        try:\n",
    "            image = cv2.imread(self.image_paths[index])\n",
    "            image = (image - self.pixel_mean) / self.pixel_std\n",
    "            print(\"ok\")\n",
    "        except ():\n",
    "            print(\"not ok\")\n",
    "            print(self.image_paths[index])\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        transforms = train_transforms(self.image_size, h, w)\n",
    "    \n",
    "        masks_list = []\n",
    "        boxes_list = []\n",
    "        # point_coords_list, point_labels_list = [], []\n",
    "        mask_path = random.choices(self.label_paths[index], k=self.mask_num)\n",
    "        for m in mask_path:\n",
    "            pre_mask = cv2.imread(m, 0)\n",
    "            if pre_mask.max() == 255:\n",
    "                pre_mask = pre_mask / 255\n",
    "\n",
    "            augments = transforms(image=image, mask=pre_mask)\n",
    "            image_tensor, mask_tensor = augments['image'], augments['mask'].to(torch.int64)\n",
    "\n",
    "            boxes = get_boxes_from_mask(mask_tensor)\n",
    "            # point_coords, point_label = init_point_sampling(mask_tensor, self.point_num)\n",
    "\n",
    "            masks_list.append(mask_tensor)\n",
    "            boxes_list.append(boxes)\n",
    "            # point_coords_list.append(point_coords)\n",
    "            # point_labels_list.append(point_label)\n",
    "\n",
    "        mask = torch.stack(masks_list, dim=0)\n",
    "        boxes = torch.stack(boxes_list, dim=0)\n",
    "        # point_coords = torch.stack(point_coords_list, dim=0)\n",
    "        # point_labels = torch.stack(point_labels_list, dim=0)\n",
    "\n",
    "        image_input[\"image\"] = image_tensor.unsqueeze(0)\n",
    "        image_input[\"label\"] = mask.unsqueeze(1)\n",
    "        image_input[\"boxes\"] = boxes\n",
    "        # image_input[\"point_coords\"] = point_coords\n",
    "        # image_input[\"point_labels\"] = point_labels\n",
    "\n",
    "        image_name = self.image_paths[index].split('/')[-1]\n",
    "        if self.requires_name:\n",
    "            image_input[\"name\"] = image_name\n",
    "            return image_input\n",
    "        else:\n",
    "            return image_input\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for medicatdatasetclass\n",
    "\n",
    "dataset = MedicalDataset(\"./dataset/image_dir\",\"./dataset/mask_dir\")\n",
    "\n",
    "for i in range (dataset.__len__) :\n",
    "    print(dataset.__getitem__(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd30691",
   "metadata": {},
   "source": [
    "# later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be67c347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 13:45:34--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.13, 3.171.22.118, 3.171.22.33, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 375042383 (358M) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_b_01ec64.pth’\n",
      "\n",
      "sam_vit_b_01ec64.pt 100%[===================>] 357.67M   267MB/s    in 1.3s    \n",
      "\n",
      "2025-11-23 13:45:36 (267 MB/s) - ‘sam_vit_b_01ec64.pth’ saved [375042383/375042383]\n",
      "\n",
      ".:\n",
      "lite-sammed2d.py  SAMMed2D-lite.ipynb  sam_vit_b_01ec64.pth  segment_anything\n",
      "\n",
      "./segment_anything:\n",
      "automatic_mask_generator.py  __init__.py  predictor.py\n",
      "build_sam.py\t\t     modeling\t  utils\n",
      "\n",
      "./segment_anything/modeling:\n",
      "common.py\t  __init__.py\t   prompt_encoder.py  transformer.py\n",
      "image_encoder.py  mask_decoder.py  sam.py\n",
      "\n",
      "./segment_anything/utils:\n",
      "amg.py\t__init__.py  onnx.py  transforms.py\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "! [ ! -f sam_vit_b_* ] &&  wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80439e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"sam_vit_b_01ec64.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6283a74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "# All rights reserved.\n",
      "\n",
      "# This source code is licensed under the license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "\n",
      "import torch\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "from .modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, Sam, TwoWayTransformer\n",
      "from .modeling.image_encoder import Adapted_Block \n",
      "from torch.nn import functional as F\n",
      "\n",
      "def build_sam_vit_h(checkpoint=None):\n",
      "    return _build_sam(\n",
      "        encoder_embed_dim=1280,\n",
      "        encoder_depth=32,\n",
      "        encoder_num_heads=16,\n",
      "        encoder_global_attn_indexes=[7, 15, 23, 31],\n",
      "        checkpoint=checkpoint,\n",
      "    )\n",
      "\n",
      "\n",
      "build_sam = build_sam_vit_h\n",
      "\n",
      "\n",
      "def build_sam_vit_l(checkpoint=None):\n",
      "    return _build_sam(\n",
      "        encoder_embed_dim=1024,\n",
      "        encoder_depth=24,\n",
      "        encoder_num_heads=16,\n",
      "        encoder_global_attn_indexes=[5, 11, 17, 23],\n",
      "        checkpoint=checkpoint,\n",
      "    )\n",
      "\n",
      "\n",
      "def build_sam_vit_b(checkpoint=None):\n",
      "    return _build_sam(\n",
      "        encoder_embed_dim=768,\n",
      "        encoder_depth=12,\n",
      "        encoder_num_heads=12,\n",
      "        encoder_global_attn_indexes=[2, 5, 8, 11],\n",
      "        checkpoint=checkpoint,\n",
      "    )\n",
      "\n",
      "\n",
      "sam_model_registry = {\n",
      "    \"default\": build_sam_vit_h,\n",
      "    \"vit_h\": build_sam_vit_h,\n",
      "    \"vit_l\": build_sam_vit_l,\n",
      "    \"vit_b\": build_sam_vit_b,\n",
      "}\n",
      "\n",
      "\n",
      "def _build_sam(\n",
      "    encoder_embed_dim,\n",
      "    encoder_depth,\n",
      "    encoder_num_heads,\n",
      "    encoder_global_attn_indexes,\n",
      "    checkpoint=None,\n",
      "):\n",
      "    prompt_embed_dim = 256\n",
      "    image_size = 1024\n",
      "    vit_patch_size = 16\n",
      "    image_embedding_size = image_size // vit_patch_size\n",
      "    sam = Sam(\n",
      "        image_encoder=ImageEncoderViT(\n",
      "            depth=encoder_depth,\n",
      "            embed_dim=encoder_embed_dim,\n",
      "            img_size=image_size,\n",
      "            mlp_ratio=4,\n",
      "            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
      "            num_heads=encoder_num_heads,\n",
      "            patch_size=vit_patch_size,\n",
      "            qkv_bias=True,\n",
      "            use_rel_pos=True,\n",
      "            global_attn_indexes=encoder_global_attn_indexes,\n",
      "            window_size=14,\n",
      "            out_chans=prompt_embed_dim,\n",
      "            transformer_block=Adapted_Block\n",
      "        ),\n",
      "        prompt_encoder=PromptEncoder(\n",
      "            embed_dim=prompt_embed_dim,\n",
      "            image_embedding_size=(image_embedding_size, image_embedding_size),\n",
      "            input_image_size=(image_size, image_size),\n",
      "            mask_in_chans=16,\n",
      "        ),\n",
      "        mask_decoder=MaskDecoder(\n",
      "            num_multimask_outputs=3,\n",
      "            transformer=TwoWayTransformer(\n",
      "                depth=2,\n",
      "                embedding_dim=prompt_embed_dim,\n",
      "                mlp_dim=2048,\n",
      "                num_heads=8,\n",
      "            ),\n",
      "            transformer_dim=prompt_embed_dim,\n",
      "            iou_head_depth=3,\n",
      "            iou_head_hidden_dim=256,\n",
      "        ),\n",
      "        pixel_mean=[123.675, 116.28, 103.53],\n",
      "        pixel_std=[58.395, 57.12, 57.375],\n",
      "    )\n",
      "    sam.eval()\n",
      "    # if checkpoint is not None:\n",
      "    #     with open(checkpoint, \"rb\") as f:\n",
      "    #         state_dict = torch.load(f)\n",
      "    #     sam.load_state_dict(state_dict)\n",
      "    if checkpoint is not None:\n",
      "        with open(checkpoint, \"rb\") as f:\n",
      "            state_dict = torch.load(f, map_location=\"cpu\")\n",
      "        try:\n",
      "            if 'model' in state_dict.keys():\n",
      "                sam.load_state_dict(state_dict['model'], False)\n",
      "            else:\n",
      "                if image_size==1024 and transformer_block != None:\n",
      "                    sam.load_state_dict(state_dict, False)\n",
      "                else:\n",
      "                    sam.load_state_dict(state_dict)\n",
      "        except:\n",
      "            print('*******interpolate')\n",
      "            new_state_dict = load_from(sam, state_dict, image_size, vit_patch_size)   \n",
      "            sam.load_state_dict(new_state_dict)\n",
      "        print(f\"*******load {checkpoint}\")\n",
      "    return sam\n",
      "\n",
      "\n",
      "\n",
      "def load_from(sam, state_dicts, image_size, vit_patch_size):\n",
      "\n",
      "    sam_dict = sam.state_dict()\n",
      "    except_keys = ['mask_tokens', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
      "    new_state_dict = {k: v for k, v in state_dicts.items() if\n",
      "                      k in sam_dict.keys() and except_keys[0] not in k and except_keys[1] not in k and except_keys[2] not in k}\n",
      "    pos_embed = new_state_dict['image_encoder.pos_embed']\n",
      "    token_size = int(image_size // vit_patch_size)\n",
      "    if pos_embed.shape[1] != token_size:\n",
      "        # resize pos embedding, which may sacrifice the performance, but I have no better idea\n",
      "        pos_embed = pos_embed.permute(0, 3, 1, 2)  # [b, c, h, w]\n",
      "        pos_embed = F.interpolate(pos_embed, (token_size, token_size), mode='bilinear', align_corners=False)\n",
      "        pos_embed = pos_embed.permute(0, 2, 3, 1)  # [b, h, w, c]\n",
      "        new_state_dict['image_encoder.pos_embed'] = pos_embed\n",
      "        rel_pos_keys = [k for k in sam_dict.keys() if 'rel_pos' in k]\n",
      "\n",
      "        global_rel_pos_keys = [k for k in rel_pos_keys if \n",
      "                                                        '2' in k or \n",
      "                                                        '5' in k or \n",
      "                                                        '7' in k or \n",
      "                                                        '8' in k or \n",
      "                                                        '11' in k or \n",
      "                                                        '13' in k or\n",
      "                                                        '15' in k or \n",
      "                                                        '23' in k or \n",
      "                                                        '31' in k] \n",
      "        # print(sam_dict)\n",
      "        for k in global_rel_pos_keys:\n",
      "            h_check, w_check = sam_dict[k].shape\n",
      "            rel_pos_params = new_state_dict[k]\n",
      "            h, w = rel_pos_params.shape\n",
      "            rel_pos_params = rel_pos_params.unsqueeze(0).unsqueeze(0)\n",
      "            if h != h_check or w != w_check:\n",
      "                rel_pos_params = F.interpolate(rel_pos_params, (h_check, w_check), mode='bilinear', align_corners=False)\n",
      "\n",
      "            new_state_dict[k] = rel_pos_params[0, 0, ...]\n",
      "\n",
      "    sam_dict.update(new_state_dict)\n",
      "    return sam_dict"
     ]
    }
   ],
   "source": [
    "! cat segment_anything/build_sam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f30768b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_build_sam() missing 1 required positional argument: 'transformer_block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2763531967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msegment_anything\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msam_model_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_model_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vit_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msam_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/segment_anything/build_sam.py\u001b[0m in \u001b[0;36mbuild_sam_vit_b\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_sam_vit_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     return _build_sam(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mencoder_embed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mencoder_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _build_sam() missing 1 required positional argument: 'transformer_block'"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from segment_anything.build_sam import sam_model_registry\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=sam_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,parm in sam.image_encoder.named_parameters() : \n",
    "    # if \"Adapter\" in name : \n",
    "        print(f\"name : {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adff752",
   "metadata": {},
   "source": [
    "## Model implementation SAM-Med2D-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6165a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMMed2DLite(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sam_model : Sam,\n",
    "        embed_dim=768,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"Lite version of SAM-Med2D with adapter layers for medical image segmentation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.sam = sam_model\n",
    "\n",
    "        #Freezing image encoder params except the adapter layer\n",
    "        for name , params in self.sam.image_encoder.named_parameters() :\n",
    "            if \"adapter\" in name.lower():\n",
    "                params.requires_grad = True\n",
    "            else : \n",
    "                params.requires_grad = False\n",
    "            \n",
    "        # Fine-tune prompt encoder and mask decoder\n",
    "        for param in self.sam.prompt_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.sam.mask_decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward (self,images,boxes) :\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            images : [B, 3, H, W]\n",
    "            boxes : [B, 4] in xyxy format\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        #image encoding \n",
    "        image_embeddings = self._encode_with_adapters(images)\n",
    "\n",
    "        # Prepare prompts (boxes)\n",
    "        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=boxes,\n",
    "            masks=None,\n",
    "        )\n",
    "        \n",
    "        # Decode masks\n",
    "        low_res_masks, iou_predictions = self.sam.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.sam.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        \n",
    "        # Upscale masks\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks,\n",
    "            size=(images.shape[2], images.shape[3]),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return masks, iou_predictions\n",
    "\n",
    "    def _encode_with_adapters(self, x):\n",
    "        \"\"\"Image encoding with adapter injection\"\"\"\n",
    "        x = self.sam.image_encoder.patch_embed(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if self.sam.image_encoder.pos_embed is not None:\n",
    "            x = x + self.sam.image_encoder.pos_embed\n",
    "        \n",
    "        # Pass through transformer blocks with adapters\n",
    "        for i, block in enumerate(self.sam.image_encoder.blocks):\n",
    "            x = block(x)\n",
    "            x = self.adapters[i](x)  # Apply adapter\n",
    "        \n",
    "        x = self.sam.image_encoder.neck(x.permute(0, 3, 1, 2))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e1162",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d021ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Dice loss for binary segmentation.\n",
    "    Args:\n",
    "        pred: logits (B, 1, H, W)\n",
    "        target: binary mask (B, 1, H, W)\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "\n",
    "    dice_score = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_score.mean()\n",
    "\n",
    "\n",
    "def focal_loss(pred, target, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    Args:\n",
    "        pred: logits (B, 1, H, W)\n",
    "        target: binary mask (B, 1, H, W)\n",
    "    \"\"\"\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "    pt = torch.exp(-bce)\n",
    "    loss = alpha * (1 - pt) ** gamma * bce\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def iou_mse_loss(pred_mask, gt_mask, pred_iou):\n",
    "    \"\"\"\n",
    "    MSE loss between predicted IoU score and true IoU.\n",
    "    Args:\n",
    "        pred_mask: logits (B, 1, H, W)\n",
    "        gt_mask: ground-truth mask (B, 1, H, W)\n",
    "        pred_iou: predicted IoU head output (B,)\n",
    "    \"\"\"\n",
    "    pred_mask = torch.sigmoid(pred_mask)\n",
    "\n",
    "    intersection = (pred_mask * gt_mask).sum(dim=(2, 3))\n",
    "    union = pred_mask.sum(dim=(2, 3)) + gt_mask.sum(dim=(2, 3)) - intersection\n",
    "\n",
    "    true_iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return F.mse_loss(pred_iou.squeeze(), true_iou)\n",
    "\n",
    "\n",
    "def combined_loss(pred, target, focal_w=20.0, dice_w=1.0):\n",
    "    \"\"\"\n",
    "    Hybrid loss combining Dice and Focal loss.\n",
    "    Matches the paper ratio: 20 (Focal) : 1 (Dice)\n",
    "    \"\"\"\n",
    "    d = dice_loss(pred, target)\n",
    "    f = focal_loss(pred, target)\n",
    "    return dice_w * d + focal_w * f\n",
    "\n",
    "\n",
    "def total_loss_fn(pred_mask, gt_mask, pred_iou):\n",
    "    \"\"\"\n",
    "    Full loss = Mask Loss (Focal+Dice) + IoU MSE loss.\n",
    "    \"\"\"\n",
    "    mask_loss = combined_loss(pred_mask, gt_mask)\n",
    "    iou_loss = iou_mse_loss(pred_mask, gt_mask, pred_iou)\n",
    "    return mask_loss + iou_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6501b",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_dice_coefficient(pred, target, threshold=0.5):\n",
    "    \"\"\"Compute Dice coefficient\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    dice = (2.0 * intersection) / union\n",
    "    return dice.item()\n",
    "\n",
    "def compute_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"Compute Intersection over Union (IoU)\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def evaluate_batch(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_dice = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            masks_gt = batch['mask'].to(device)\n",
    "            boxes = batch['bbox'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            masks_pred, _ = model(images, boxes)\n",
    "            \n",
    "            # Compute metrics\n",
    "            for i in range(images.shape[0]):\n",
    "                dice = compute_dice_coefficient(masks_pred[i], masks_gt[i])\n",
    "                iou = compute_iou(masks_pred[i], masks_gt[i])\n",
    "                total_dice += dice\n",
    "                total_iou += iou\n",
    "                num_samples += 1\n",
    "    \n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_iou = total_iou / num_samples\n",
    "    \n",
    "    return {'dice': avg_dice, 'iou': avg_iou}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bebb8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d19b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        masks_gt = batch['mask'].to(device)\n",
    "        boxes = batch['bbox'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        masks_pred, iou_pred = model(images, boxes)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = combined_loss(masks_pred, masks_gt)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-4,\n",
    "    save_dir='checkpoints'\n",
    "):\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Optimizer (only trainable parameters)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs\n",
    "    )\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    history = {'train_loss': [], 'val_dice': [], 'val_iou': []}\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = evaluate_batch(model, val_loader, device)\n",
    "        history['val_dice'].append(val_metrics['dice'])\n",
    "        history['val_iou'].append(val_metrics['iou'])\n",
    "        \n",
    "        # Learning rate step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Dice: {val_metrics['dice']:.4f}\")\n",
    "        print(f\"Val IoU: {val_metrics['iou']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['dice'] > best_dice:\n",
    "            best_dice = val_metrics['dice']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice': best_dice,\n",
    "            }, f\"{save_dir}/best_model.pth\")\n",
    "            print(f\"Saved best model with Dice: {best_dice:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f998a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "class InferencePredictor:\n",
    "    \"\"\"Simple inference wrapper\"\"\"\n",
    "    def __init__(self, model, device, image_size=256):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    def predict(self, image_path: str, bbox: List[int], threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict segmentation mask\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            bbox: Bounding box [x1, y1, x2, y2]\n",
    "            threshold: Prediction threshold\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_size = image.shape[:2]\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(self.device)\n",
    "        bbox_tensor = torch.tensor([bbox], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            mask_pred, iou_pred = self.model(image_tensor, bbox_tensor)\n",
    "            mask_pred = torch.sigmoid(mask_pred[0, 0])\n",
    "        \n",
    "        # Post-process\n",
    "        mask = (mask_pred > threshold).cpu().numpy().astype(np.uint8)\n",
    "        mask = cv2.resize(mask, (original_size[1], original_size[0]), \n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return mask, iou_pred.item()\n",
    "    \n",
    "    def visualize_prediction(self, image_path: str, bbox: List[int], \n",
    "                            save_path: Optional[str] = None):\n",
    "        \"\"\"Visualize prediction result\"\"\"\n",
    "        # Predict\n",
    "        mask, iou = self.predict(image_path, bbox)\n",
    "        \n",
    "        # Load original image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image with bbox\n",
    "        axes[0].imshow(image)\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "                             fill=False, color='red', linewidth=2)\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].set_title('Input + BBox')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Predicted mask\n",
    "        axes[1].imshow(mask, cmap='gray')\n",
    "        axes[1].set_title(f'Predicted Mask (IoU: {iou:.3f})')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = image.copy()\n",
    "        overlay[mask > 0] = [255, 0, 0]\n",
    "        blended = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)\n",
    "        axes[2].imshow(blended)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
