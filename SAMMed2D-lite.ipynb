{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c383648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad31701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "filelist=(\"sam_vit_b_01ec64.pth\" \"sample_data\")\n",
    "\n",
    "pat=$(printf \"^%s$\" \"${filelist[@]}\")\n",
    "pat=${pat:1}\n",
    "\n",
    "\n",
    "ls | grep -Ev \"$pat\" | xargs rm -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ea8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13411f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: copied repo content into ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp_repo'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Variables\n",
    "REPO_URL=\"https://github.com/LIMAMMohamedlimam/sammed-lite.git\"\n",
    "CLONE_DIR=\"temp_repo\"\n",
    "TARGET_DIR=\"./\"\n",
    "git clone \"$REPO_URL\" \"$CLONE_DIR\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "mkdir -p \"$TARGET_DIR\"\n",
    "\n",
    "# Copy all contents (including hidden files)\n",
    "cp -r \"$CLONE_DIR\"/. \"$TARGET_DIR\"/\n",
    "\n",
    "# Delete cloned repo directory\n",
    "rm -rf \"$CLONE_DIR\"\n",
    "\n",
    "echo \"Done: copied repo content into $TARGET_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ef5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "lite-sammed2d.py  SAMMed2D-lite.ipynb  segment_anything\n",
      "\n",
      "./segment_anything:\n",
      "automatic_mask_generator.py  __init__.py  predictor.py\n",
      "build_sam.py\t\t     modeling\t  utils\n",
      "\n",
      "./segment_anything/modeling:\n",
      "common.py\t  __init__.py\t   prompt_encoder.py  transformer.py\n",
      "image_encoder.py  mask_decoder.py  sam.py\n",
      "\n",
      "./segment_anything/utils:\n",
      "amg.py\t__init__.py  onnx.py  transforms.py\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e35691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling import (\n",
    "    Sam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84aa32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8670e",
   "metadata": {},
   "source": [
    "## Medical Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebadc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "import random\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def train_transforms(img_size, ori_h, ori_w):\n",
    "    transforms = []\n",
    "    if ori_h < img_size and ori_w < img_size:\n",
    "        transforms.append(A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)))\n",
    "    else:\n",
    "        transforms.append(A.Resize(int(img_size), int(img_size), interpolation=cv2.INTER_NEAREST))\n",
    "    transforms.append(ToTensorV2(p=1.0))\n",
    "    return A.Compose(transforms, p=1.)\n",
    "\n",
    "\n",
    "def get_boxes_from_mask(mask, box_num=1, std = 0.1, max_pixel = 5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mask: Mask, can be a torch.Tensor or a numpy array of binary mask.\n",
    "        box_num: Number of bounding boxes, default is 1.\n",
    "        std: Standard deviation of the noise, default is 0.1.\n",
    "        max_pixel: Maximum noise pixel value, default is 5.\n",
    "    Returns:\n",
    "        noise_boxes: Bounding boxes after noise perturbation, returned as a torch.Tensor.\n",
    "    \"\"\"\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.numpy()\n",
    "        \n",
    "    label_img = label(mask)\n",
    "    regions = regionprops(label_img)\n",
    "\n",
    "    # Iterate through all regions and get the bounding box coordinates\n",
    "    boxes = [tuple(region.bbox) for region in regions]\n",
    "\n",
    "    # If the generated number of boxes is greater than the number of categories,\n",
    "    # sort them by region area and select the top n regions\n",
    "    if len(boxes) >= box_num:\n",
    "        sorted_regions = sorted(regions, key=lambda x: x.area, reverse=True)[:box_num]\n",
    "        boxes = [tuple(region.bbox) for region in sorted_regions]\n",
    "\n",
    "    # If the generated number of boxes is less than the number of categories,\n",
    "    # duplicate the existing boxes\n",
    "    elif len(boxes) < box_num:\n",
    "        num_duplicates = box_num - len(boxes)\n",
    "        boxes += [boxes[i % len(boxes)] for i in range(num_duplicates)]\n",
    "\n",
    "    # Perturb each bounding box with noise\n",
    "    noise_boxes = []\n",
    "    for box in boxes:\n",
    "        y0, x0,  y1, x1  = box\n",
    "        width, height = abs(x1 - x0), abs(y1 - y0)\n",
    "        # Calculate the standard deviation and maximum noise value\n",
    "        noise_std = min(width, height) * std\n",
    "        max_noise = min(max_pixel, int(noise_std * 5))\n",
    "         # Add random noise to each coordinate\n",
    "        try:\n",
    "            noise_x = np.random.randint(-max_noise, max_noise)\n",
    "        except:\n",
    "            noise_x = 0\n",
    "        try:\n",
    "            noise_y = np.random.randint(-max_noise, max_noise)\n",
    "        except:\n",
    "            noise_y = 0\n",
    "        x0, y0 = x0 + noise_x, y0 + noise_y\n",
    "        x1, y1 = x1 + noise_x, y1 + noise_y\n",
    "        noise_boxes.append((x0, y0, x1, y1))\n",
    "    return torch.as_tensor(noise_boxes, dtype=torch.float)\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=256, mode='train', requires_name=True, point_num=1, mask_num=5):\n",
    "        \"\"\"\n",
    "        Initializes a training dataset.\n",
    "        Args:\n",
    "            data_dir (str): Directory containing the dataset.\n",
    "            image_size (int, optional): Desired size for the input images. Defaults to 256.\n",
    "            mode (str, optional): Mode of the dataset. Defaults to 'train'.\n",
    "            requires_name (bool, optional): Indicates whether to include image names in the output. Defaults to True.\n",
    "            num_points (int, optional): Number of points to sample. Defaults to 1.\n",
    "            num_masks (int, optional): Number of masks to sample. Defaults to 5.\n",
    "        \"\"\"\n",
    "        self.image_size = image_size\n",
    "        self.requires_name = requires_name\n",
    "        self.point_num = point_num\n",
    "        self.mask_num = mask_num\n",
    "        self.pixel_mean = [123.675, 116.28, 103.53]\n",
    "        self.pixel_std = [58.395, 57.12, 57.375]\n",
    "\n",
    "        dataset = json.load(open(os.path.join(data_dir, f'image2label_{mode}.json'), \"r\"))\n",
    "        for i in list(dataset.keys()):\n",
    "            self.image_paths.append(i.replace(\"data_demo/images/\" , \"images_dir/\")) \n",
    "        self.label_paths = list(dataset.values())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "        Returns:\n",
    "            dict: A dictionary containing the sample data.\n",
    "        \"\"\"\n",
    "\n",
    "        image_input = {}\n",
    "        try:\n",
    "            image = cv2.imread(self.image_paths[index])\n",
    "            image = (image - self.pixel_mean) / self.pixel_std\n",
    "            print(\"ok\")\n",
    "        except ():\n",
    "            print(\"not ok\")\n",
    "            print(self.image_paths[index])\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        transforms = train_transforms(self.image_size, h, w)\n",
    "    \n",
    "        masks_list = []\n",
    "        boxes_list = []\n",
    "        # point_coords_list, point_labels_list = [], []\n",
    "        mask_path = random.choices(self.label_paths[index], k=self.mask_num)\n",
    "        for m in mask_path:\n",
    "            pre_mask = cv2.imread(m, 0)\n",
    "            if pre_mask.max() == 255:\n",
    "                pre_mask = pre_mask / 255\n",
    "\n",
    "            augments = transforms(image=image, mask=pre_mask)\n",
    "            image_tensor, mask_tensor = augments['image'], augments['mask'].to(torch.int64)\n",
    "\n",
    "            boxes = get_boxes_from_mask(mask_tensor)\n",
    "            # point_coords, point_label = init_point_sampling(mask_tensor, self.point_num)\n",
    "\n",
    "            masks_list.append(mask_tensor)\n",
    "            boxes_list.append(boxes)\n",
    "            # point_coords_list.append(point_coords)\n",
    "            # point_labels_list.append(point_label)\n",
    "\n",
    "        mask = torch.stack(masks_list, dim=0)\n",
    "        boxes = torch.stack(boxes_list, dim=0)\n",
    "        # point_coords = torch.stack(point_coords_list, dim=0)\n",
    "        # point_labels = torch.stack(point_labels_list, dim=0)\n",
    "\n",
    "        image_input[\"image\"] = image_tensor.unsqueeze(0)\n",
    "        image_input[\"label\"] = mask.unsqueeze(1)\n",
    "        image_input[\"boxes\"] = boxes\n",
    "        # image_input[\"point_coords\"] = point_coords\n",
    "        # image_input[\"point_labels\"] = point_labels\n",
    "\n",
    "        image_name = self.image_paths[index].split('/')[-1]\n",
    "        if self.requires_name:\n",
    "            image_input[\"name\"] = image_name\n",
    "            return image_input\n",
    "        else:\n",
    "            return image_input\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adff752",
   "metadata": {},
   "source": [
    "## Model implementation SAM-Med2D-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6165a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMMed2DLite(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sam_model : Sam,\n",
    "        embed_dim=768,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"Lite version of SAM-Med2D with adapter layers for medical image segmentation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.sam = sam_model\n",
    "\n",
    "        #Freezing image encoder params except the adapter layer\n",
    "        for name , params in self.sam.image_encoder.named_parameters() :\n",
    "            if \"adapter\" in name.lower():\n",
    "                params.requires_grad = True\n",
    "            else : \n",
    "                params.requires_grad = False\n",
    "            \n",
    "        # Fine-tune prompt encoder and mask decoder\n",
    "        for param in self.sam.prompt_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.sam.mask_decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward (self,images,boxes) :\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            images : [B, 3, H, W]\n",
    "            boxes : [B, 4] in xyxy format\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        #image encoding \n",
    "        image_embeddings = self.sam.image_encoder.forward(images)\n",
    "\n",
    "        # Prepare prompts (boxes)\n",
    "        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=boxes,\n",
    "            masks=None,\n",
    "        )\n",
    "        \n",
    "        # Decode masks\n",
    "        low_res_masks, iou_predictions = self.sam.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.sam.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        \n",
    "        # Upscale masks\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks,\n",
    "            size=(images.shape[2], images.shape[3]),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return masks, iou_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e1162",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d021ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Dice loss for binary segmentation.\n",
    "    Args:\n",
    "        pred: logits (B, 1, H, W)\n",
    "        target: binary mask (B, 1, H, W)\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "\n",
    "    dice_score = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_score.mean()\n",
    "\n",
    "\n",
    "def focal_loss(pred, target, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    Args:\n",
    "        pred: logits (B, 1, H, W)\n",
    "        target: binary mask (B, 1, H, W)\n",
    "    \"\"\"\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "    pt = torch.exp(-bce)\n",
    "    loss = alpha * (1 - pt) ** gamma * bce\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def iou_mse_loss(pred_mask, gt_mask, pred_iou):\n",
    "    \"\"\"\n",
    "    MSE loss between predicted IoU score and true IoU.\n",
    "    Args:\n",
    "        pred_mask: logits (B, 1, H, W)\n",
    "        gt_mask: ground-truth mask (B, 1, H, W)\n",
    "        pred_iou: predicted IoU head output (B,)\n",
    "    \"\"\"\n",
    "    pred_mask = torch.sigmoid(pred_mask)\n",
    "\n",
    "    intersection = (pred_mask * gt_mask).sum(dim=(2, 3))\n",
    "    union = pred_mask.sum(dim=(2, 3)) + gt_mask.sum(dim=(2, 3)) - intersection\n",
    "\n",
    "    true_iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return F.mse_loss(pred_iou.squeeze(), true_iou)\n",
    "\n",
    "\n",
    "def combined_loss(pred, target, focal_w=20.0, dice_w=1.0):\n",
    "    \"\"\"\n",
    "    Hybrid loss combining Dice and Focal loss.\n",
    "    Matches the paper ratio: 20 (Focal) : 1 (Dice)\n",
    "    \"\"\"\n",
    "    d = dice_loss(pred, target)\n",
    "    f = focal_loss(pred, target)\n",
    "    return dice_w * d + focal_w * f\n",
    "\n",
    "\n",
    "def total_loss_fn(pred_mask, gt_mask, pred_iou):\n",
    "    \"\"\"\n",
    "    Full loss = Mask Loss (Focal+Dice) + IoU MSE loss.\n",
    "    \"\"\"\n",
    "    mask_loss = combined_loss(pred_mask, gt_mask)\n",
    "    iou_loss = iou_mse_loss(pred_mask, gt_mask, pred_iou)\n",
    "    return mask_loss + iou_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6501b",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_dice_coefficient(pred, target, threshold=0.5):\n",
    "    \"\"\"Compute Dice coefficient\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    dice = (2.0 * intersection) / union\n",
    "    return dice.item()\n",
    "\n",
    "def compute_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"Compute Intersection over Union (IoU)\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def evaluate_batch(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_dice = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            masks_gt = batch['mask'].to(device)\n",
    "            boxes = batch['bbox'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            masks_pred, _ = model(images, boxes)\n",
    "            \n",
    "            # Compute metrics\n",
    "            for i in range(images.shape[0]):\n",
    "                dice = compute_dice_coefficient(masks_pred[i], masks_gt[i])\n",
    "                iou = compute_iou(masks_pred[i], masks_gt[i])\n",
    "                total_dice += dice\n",
    "                total_iou += iou\n",
    "                num_samples += 1\n",
    "    \n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_iou = total_iou / num_samples\n",
    "    \n",
    "    return {'dice': avg_dice, 'iou': avg_iou}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bebb8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d19b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        masks_gt = batch['mask'].to(device)\n",
    "        boxes = batch['bbox'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        masks_pred, iou_pred = model(images, boxes)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = combined_loss(masks_pred, masks_gt)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-4,\n",
    "    save_dir='checkpoints'\n",
    "):\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Optimizer (only trainable parameters)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs\n",
    "    )\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    history = {'train_loss': [], 'val_dice': [], 'val_iou': []}\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = evaluate_batch(model, val_loader, device)\n",
    "        history['val_dice'].append(val_metrics['dice'])\n",
    "        history['val_iou'].append(val_metrics['iou'])\n",
    "        \n",
    "        # Learning rate step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Dice: {val_metrics['dice']:.4f}\")\n",
    "        print(f\"Val IoU: {val_metrics['iou']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['dice'] > best_dice:\n",
    "            best_dice = val_metrics['dice']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice': best_dice,\n",
    "            }, f\"{save_dir}/best_model.pth\")\n",
    "            print(f\"Saved best model with Dice: {best_dice:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f998a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "class InferencePredictor:\n",
    "    \"\"\"Simple inference wrapper\"\"\"\n",
    "    def __init__(self, model, device, image_size=256):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    def predict(self, image_path: str, bbox: List[int], threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict segmentation mask\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            bbox: Bounding box [x1, y1, x2, y2]\n",
    "            threshold: Prediction threshold\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_size = image.shape[:2]\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(self.device)\n",
    "        bbox_tensor = torch.tensor([bbox], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            mask_pred, iou_pred = self.model(image_tensor, bbox_tensor)\n",
    "            mask_pred = torch.sigmoid(mask_pred[0, 0])\n",
    "        \n",
    "        # Post-process\n",
    "        mask = (mask_pred > threshold).cpu().numpy().astype(np.uint8)\n",
    "        mask = cv2.resize(mask, (original_size[1], original_size[0]), \n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return mask, iou_pred.item()\n",
    "    \n",
    "    def visualize_prediction(self, image_path: str, bbox: List[int], \n",
    "                            save_path: Optional[str] = None):\n",
    "        \"\"\"Visualize prediction result\"\"\"\n",
    "        # Predict\n",
    "        mask, iou = self.predict(image_path, bbox)\n",
    "        \n",
    "        # Load original image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image with bbox\n",
    "        axes[0].imshow(image)\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "                             fill=False, color='red', linewidth=2)\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].set_title('Input + BBox')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Predicted mask\n",
    "        axes[1].imshow(mask, cmap='gray')\n",
    "        axes[1].set_title(f'Predicted Mask (IoU: {iou:.3f})')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = image.copy()\n",
    "        overlay[mask > 0] = [255, 0, 0]\n",
    "        blended = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)\n",
    "        axes[2].imshow(blended)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
